1.1 AI vs Machine Learning vs Deep Learning | AI vs ML vs DL | Machine Learning Training with Python
Artificial Intelligence
is branch of computer science that is concerned with building smart & intelligent machines
Machine Learning is a subset of AI
is a technique to implement AI that can learn from the data by themselves without being explicitly programmed
Deep Learning is a subset of ML
is a subfield of ML that uses a special type of algorithms called as Artifical Neural Networks to learn from the data

1.2. Supervised vs Unsupervised vs Reinforcement Learning | Types of Machine Learning
Supervised Learning is a type of ML
in Supervised Learning, the ML algorithem learns from Labelled Data
    we give the algorithm some picture of appels and label it as appel and some picture of mango and label it as mango the algorithem tries to find the patterns between these images when we make a picture of unanimous image the algo try to give with the patterns the name
Unsupervised Learning is a type of ML
in Unsupervised Learning, the ML algorithem learns from Unlabelled Data
    we give the algorithem some picture the algorithem find the pattern and it tries to group these picture and when we give a new image the algo class this new picture in a group
Reinforcement Learning is a type of ML
is an area of machine learning concerned with how intelligent agents take actions in an envirenment to maximize its rewards (Environment, Agent, Action, Reward)

1.3. Supervised Learning | Types of Supervised Learning | Machine Learning Tutorial
Classification is a type of Supervised Learning
is about predicting a class or discrete values, male/female, appel/mango, true/false ...
Regression is a type of Supervised Learning
is about predicting a quantity or continuous values, age, salary, price ...

Logistic Regression is a type of Classification
Support Vector Machine Classifier is a type of Classification
Decision Tree Classification is a type of Classification
K-nearest Neighbor is a type of Classification
Random Forest Classification is a type of Classification
Naive Bayes Classifier is a type of Classification

Linear Regression is type of Regression
Lasso Regression is type of Regression
Polynomial Regression is type of Regression
Support Vector Machines Regressor is type of Regression
Random Forest Regressor is type of Regression
Bayesin Linear Regressor is type of Regression

1.4. Unsupervised Learning | Clustering and Association Algorithms in Machine Learning
Clustering is a type of Unsupervised Learning
is an unsupervised task which involves grouping the similar data points
Association is a type of Unsupervised Learning
is an unsupervised task that used to find important relationship between data points
    customer1 bay milk, bread, jus, customer2 bay mlik, bread, soda, customer3 bay milk, ? => bread 

K-Means Clustering is type of Clustering
Hierarchical Clustering is type of Clustering
Pricipal Component Analysis (PCA) is type of Clustering

Apriori is type of Association
Eclat is type of Association


1.5. What is Deep Learning | Deep Learning Tutorial | Deep Learning Simplified
ML
Input -> Features extraction by humain -> classification -> Output

DL
Input -> Features extraction automatically -> classification -> Output

2.
Where to collect the Data? 
    Kaggle
    UCI Machine Learning Repository
    Google Dataset Search

ML Project Work Flow
   Data
-> Data pre processing (eg. handle missing values)
-> Data Analysis (eg. find which columns feature is important for the prediction)
-> Train Test Split (Split data into training data and testing data)
-> Machine Learning Model (feed the training data to ML model to find pattern)
-> Evaluation (Evaluate the model using the test data)

Handling Missing Values : replace by mean (average), median (central), mode (more repeated) 

Data Standardization : make all the data to a common format and common range

Label Encoding : converting the labels into numeric form

Train Test Split : splitting the data into training data and test data

Handle Imbalanced Data : building a dataset containing similar distribution of different

Feature Extraction : the mapping from textual data to real valued vwctor is called feature extraction
    1. Bag Of Words (BOW) : list of unique words in the text corpus
    2. Term Frequency-Inverse Document Frequency (TF-IDF) : To count the number of times each word appears in a document
        Term Frequency (TF) : Number of times term t appears in a document / Number of terms in the document
        Inverse Docuement Frequency (IDF) : log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in
        The IDF value of a rare word is high whereas the IDF of a frequent word is low
            TF-IDF value of term = TF * IDF

Stemming  : is the process of reducing a word to its keyword (Root word)

5.0 Mathematics for Machine Learning
    5.1 Linear Algebra
    5.2 Statistics
    5.3 Probability
    5.4 calculs

5.1.1 Linear Algebra Vectors
        Speed = 50 km/hr (Scalar Quantity)
        Velocity = 50 km/hr North (Vector Quantity)

5.1.2 Vector Operation Part 1
5.1.3 Vector Operation In Python Part 1
        Vector have : Vector : (2, 2) or 2i + 2j
            1. Magnitude = sqr(exp(x1, 2),exp(x2, 2)) = sqr(exp(2,2) + exp(2,2)) = sqr(4+4) = sqr(8)
            2. Direction = tan (a) = 2/2 = 1 (a = $%)
        
        Vector Addition = (2,3) + (3,-2) = (5,1)
        Vector Subtraction = (2,3) + (3,-2) = (-1,5)
        Multiplying a Vector by Scalar = (2,3) * 2 = (4,6)
        Multiplying a Vector by Scalar = (2,3) * -0.5 = (-1,-1.5)
5.1.4 Vector Operation Part 1
5.1.5 Vector Operation In Python Part 1
        Angle between 2 Vectors :
            Inference : 
                If the angle between 2 vectors is small then the 2 vectors are similar
                If the angle between 2 vectors is large then the 2 vectors are very different
        Dot Product of 2 Vectors = (2,3) . (4,4) = (2 * 4) + (3 * 4) = 20
        Cross Product of 2 Vectors = (2,3) * (4,4) = ([2, 3 , 0] ,[4, 4, 0]) = 
                                = i(3*0 - 4*0) - j(2*0 - 4*0) + k (2*4 - 3*4)
                                = i(0-0) - j(0-0) - k(8-12) = {0,0,-4}
        Projection of Vector = proj(v,a) = a.v/exp(||v||,2) * v
5.1.6 Matrix
5.1.7 Matrix In Python
        Null Matrix or Zero Matrix (all values are zero)
        Identity Matrix (all of diagonal values are one and all the remaining values are zero)
        Transpose of a matrix is formed by turning all the rows of a given matrix into columns and vice-versa
5.1.8 Matrix Operation
5.1.9 Matrix Operation In Python
        Matrix Addition
        Two matrices can be added only if they have the same shape, 
        that is both the matrix should have the same number of rows and columns
        [[2,3],[10,5]] + [[10,5],[20,4]] = [[12,8],[30,9]]
        Martix Subtraction
        Two matrices can be subtracted only if they have the same shape, 
        that is both the matrix should have the same number of rows and columns
        [[2,3],[10,5]] - [[10,5],[20,4]] = [[-8,-2],[-10,1]]
        Multiplying a Martix by Scalar
        5 * [[2,1],[4,2],[6,3]] = [[10,5],[20,10],[30,15]]
        Multiplying 2 Martices
        The number of columns in the first matrix should be equal to the number of rows in the second matrix
        The resultant matrix will have the same number of rows as the first matrix and same number of columns as the second matrix
        [[2,3],[8,6]] * [[10,5],[20,4]] = [[2*10+3*20,2*5+3*4],[8*10+6*20,8*6+5*4]] = [[80,22],[200,68]]
    

5.2.1 Statistics
is the science concerned with developing and studying methods for collection analysing interpreting and presenting data
    1. Range
    2. Mean
    3. Standard Devistion

5.2.2.Basics of Statistics
    Type of data :
        Categorical :
            Nominal : eg. Male , Female (there is no order)
            is a classification of categorical variables that do not provide any quantitative value
            Ordianel : eg. very bad, bad, good, very good (there is order)
            are the type of data in which the values follow a natural order
        Numerical :
            Discrete : 
            are the type of data that can only take certain values
            Continuous : 
            can have almost any numeric value unlike discrete data they can have decimal values
5.2.3.Types of Statistics
    Types of Statistics :
        Descriptive Statistics : 
        are used to describe the basic features of the data in a study they provide simple summaries about the sample and the measures
            Describe the data
            Using in Data Analysis
            eg. What is the total investment made by the company
            Use numerical measure
            2 Important measures of descriptive statistics
                1. Measure of Central Tendencies (Mean, Median, Mode)
                2. Measure of Variability (Range, Standard Deviation, Variance)
        Inferential Statistics : 
        takes data from a sample and makes inferences and predictions about the larger population from which the sample was drawn
            Get inferences about the data
            Using in Data Science
            eg. suggest the management of the company of various strategies
            Find the inferences from data
            eg. Correlation
5.2.4.Types of Statistical Studies
    Types of Statistical Studies
        Sample Study :
        is a study which is carried out an a sample which represents the total population
        Observational Study
        is study where we simply collect and analyze data we won't inject any changes we just observe the correlation in the data
        Experimental Study
        is study in which conditions are controlled and manipulated by the experimenter
5.2.5.Population & Sample
    Population & Sample
    a sample study is a study which is carried out an a sample which represents the total population
        Sampling Techniques
            (Probability Sampling Techniques)
                Simple Random Sampling
                    the sample is randomly picked from a larger population hence all the individual datapoint has an equal probability to be selected as sample data
                    eg. employee survey in a company
                    Pros: 
                    No sample Bis
                    Balanced Sample
                    Simple Methode of Sampling
                    Requires less domain knowledge
                    Cons:
                    Population size should be high
                    Cannot represend the population well sometimes
                Systematic Sampling
                    the sample is picked from the population at regular intervals, this type of sampling is carried out if the poplation is homogeneous and the data points are uniformly distributed
                    eg. selecting every 10th member from a population of 10000
                    Pros:
                    Quick & easy
                    Less Bias
                    Even distribution of data
                    Cons:
                    Data manipulation risk
                    Requires randomness in data
                    Population should not have patterns
                Stratified Random Sampling
                    the population is subdivided into smaller groups called Strata, Samples are obtained randomly from all these strata
                    eg. Smarthphone sales in all the states
                    Props:
                    Finds important characteristics in the population
                    High precision can be obtained in the differences in the strata in high
                    Cons:
                    Cannot be performed on populations that cannot be classified into groups
                    Overlapping data points
                Cluster Sampling
                    is carried out on population that has inherent groups , this population is subdivided into clusters and then random are taken as sample
                    eg. Smartphone sales in randomly selected states
                    Pros:
                    Requires only fewer resources
                    Reduced Variability
                    Advantages of both Random sampling and Stratified sampling
                    Cons:
                    Cannot be performed on populations without natural groups
                    Overlapping data points
                    Can't provide a general insight for the entire population
            (Non-Probability Sampling Techniques)
                ...
5.2.6.Central Tendencies
    Central Tendency
    a measure of central tendency is a value that represents the center point or typical value of a dataset, it is a value that summarizes the data
        Mean (average) : is the sum of values divided by the number of values
            eg. [160,172,165,168,174], mean = (160+172+165+168+174)/5 = 167.8
        Median : is the middle value in the list of numbers, To find the median , the nubers have to be listed in numerical order from smallest to largest
            eg. [160,172,165,168,174], median = [160,165,168,172,174] = 168
            eg. [160,172,165,168,174,176], median = [160,165,168,172,174,176] = (168+172)/2 = 170
        Mode : is the value that occurs most often if no number in the list is repeated the there is no mode for the list
            eg. [160,172,165,168,174], mode = there is no mode for the list
            eg. [160,172,160,168,174], mode = 160
        Central Tendencies in Data Pre-Processing : are very useful in handling the missing values in a dataset
            mean : missing values in a dataset can be replaced with mean value if the data is uniformly distributed
            median : missing values in a dataset can be replaced with median value if the data is skewed
            mode : missing values in a dataset can be replaced with mode value if the data is skewed, missing categorical values can also be replaced with mode value
5.2.7.Range Sample Variance & Standard Deviation
    Measure of Variability
        Range : is the difference between the largest and smallest values it can give a rough idea about the distribution of our dataset
            eg. [-5,0,5,10,15], mean = 5, range = 15 - (-5) = 20
            eg. [3,4,5,6,7], mean = 5, range = 7 - 3 = 4
        Variance : is a measure of how far each number in the set is from the mean and therefore from every other number is the dataset
            eg. [-5,0,5,10,15], mean = 5, variance = exp((-5-5),2)+exp((0-5),2)+exp((5-5),2)+exp((10-5),2)+exp((15-5),2) / 5 = 50
            eg. [3,4,5,6,7], mean = 5, variance = exp((3-5),2)+exp((4-5),2)+exp((5-5),2)+exp((6-5),2)+exp((7-5),2) / 5 = 2
        Standard Deviation : is the sqaure root of variance. it looks at how spread out a group of numbers is from the mean
            eg. [-5,0,5,10,15], mean = 5, SD = sqrt(variance) = sqrt(50) = 7.1
            eg. [3,4,5,6,7], mean = 5, SD = sqrt(variance) = sqrt(2) = 1.4    
5.2.8.Percentiles & Dispersion
    Percentiles and Quantiles
        Percentiles : is a value on scale of 100 that indicates the percent of a distribution is equal to or below it
        eg. Dataset with height of 15 people, range [160,180], median = 170 is 50th Percentile, 
        Quantiles : is a measure that tells how many values in a dataset are above or below a certain limit it divides the members of the dataset into equally-sized subgroups
        eg. Dataset with height of 15 people, range [160,180], median = 170 is 0.5 or 50% Quantile, 
5.2.9.Correlation & Causation
    Correlation and Causation
        Correlation : is a measure that determines the extent to which two variables are related to each other in a dataset, But it doesn't mean that one event is the cause of the other event
            Positive Correlation : if one value increase the other value increase
            eg. house price and number of rooms
            Negative Correlation : if one value increase the other value decrease
            eg. house price and crime rate
        Causation : means that one event causes another event to occur thus there is a cause and effect relationship between two variables in a dataset
            eg. Ice cream sales and temperature
5.2.10.Hypothesis Testing
    Hypothesis : is an assumption that is made based on the observation of an experiment
        Null Hypothesis : H0 is the commonly accepted fact
        eg. Ptolemy proposed that sun, stars and other planets revolve around the earth
        Alternative Hypothesis : Ha is opposite to null hypothesis and it challenges the null hypothesis
        eg. Aryabhata proposed that earth and other planets revolve around the sun
    Hypothesis Testing : is an method carried out to tests the assumptions made in the experiment
        eg. Pharmaceutical company, headace, time taken for recovery in minutes
        drug1 for group1 : [12, 8, 13, 10, 7] = average = 10 minutes
        drug2 for group2 : [15, 12, 18, 16, 14] = average = 15 minutes
        Null Hypothesis H0 Drug1 is more quicker than drug2
        Alternative Hypothesis Ha drug2 is more quicker than drug1
        Possible Outcomes of Hypothesis Testing : 
            Reject the Null Hypothesis
            Faik to reject the Null Hypothesis
5.3.1. Probability for Machine Learning
    Probability : is branch of Mathematics that deals with calculating the likelihood of a given event to occur
    eg. Roll a dice, Toss a coin, Bag containing different coloured balls
5.3.2. Basics of Probability
    eg. what is the probability of getting a number greater than 10 when we roll a die = 0
    eg. what is the probability of getting a number less than 10 when we roll a die = 1
    Probability Value :
        impossible = 0, 0%
        half chance = 0.5, 50%, 1/2
        certain = 1, 100%
    Probability of an event to occur = Number of ways an event can occur / Total number of outcomes
        eg. Toss a coin = Possible Outcomes (H,T) = P(H) = 1/2 = P(T) = 1/2
        eg. Roll a dice = Possible Outcomes (1,2,3,4,5,6) = P(5) = 1/6 = 0.16
        eg. Roll a dice = Possible Outcomes (1,2,3,4,5,6) = P(even) = 3/6 = 0.5    
5.3.3. Random Variables and its types | Discrete Random Variables | Continuous Random Variables
    Random Variables : is a numerical description of the outcomes of random events
        eg. Toss a coin = Random events (Head, Tail) = Possible Values (0, 1) = Random Values X, X = 0 if Head and X = 1 if Tail,
        eg. Y = Weight of random person in a class, P(Weight of a random person in a class is less than 60 kg), P(Y < 60)
    Applications : 
        Turnover of a company is a given time period
        Price change of an asset over a given time period
    
    Types of Random Variables
        1. Discrete Random Variables : takes only discrete or distinct values
            eg. coin toss, colour of the ball
        2. Continuous Random Variables : take any value in a given range
            eg. weight of a random person in a class
5.3.4. Probability Distribution for Random Variable
    Probability Distribution : the probability distribution for a random variable describes how the probabilities are distributed over the values of the random variable
    eg. Tossing 3 coins : X = Sum of number of Heads when 3 coins are tossed / (HHH, HHT, HTH, THH, TTH, THT, HTT, TTT) / (3, 2, 2, 2, 1, 1, 1, 0) / P(X) = (P(3) = 1/8, P(2) = 3/8, P(1) = 3/8, P(0) = 1/8)
5.3.5. Normal Distribution or Gaussian Distribution | Skewness
    Normal Distribution : is an arrangement of a data set in which most of the data points lie in the middle of the range and the rest taper of symmetrically toward either extreme
    Normal Distribution or Gaussian Distribution
    Skewness : a data is considered skewed when the distribution curve appear distored or skewed either to the left or to the right in a statistical distribution
    Negatively skewed : Mean, Median, Mode
    Normal or no skew : Mean=Median=Mode
    Positively skewed : Mode, Median, Mean
5.3.6. Poisson Distribution
    Poisson Distribution is a probability distribution that measures how many times an event is likely to occur within a specified period of time
    Poisson Distribution is used to understand independent events that occur at a contant rate within a given interval of time
    eg. Number of accidents occurring in a city from 6pm to 10pm
    eg. Number of Patients arriving in an Emergency Room between 10pm to 12pm
    eg. How may viens does your blog gets in a day
    x -> number of times the event occurs
    P(x) -> probability
    L -> Mean number of event
    x! -> Factorial of x
    e -> Euler's Number (2.71828)
    P(x) = exp(e,-L) * exp(L,x) / x!
Maximum Likelihood
Bayes Theorem
Information Theory
Cross Entropy
Information Gain

6. Machine Learning Models
6.1. What is a Machine Learning Model?
    A machine learning model is a function that tries to find the relationship between the features and the target variable
    It tries to find the pattern in the data understand the data and tains on the data. based on this learning, machine learning model makes predictions and recognize patterns

    Inference : the above line equation is a function that relates X and Y, for a given value of X we can find the corresponding value of Y
    1.Simple linear model : Equation of a Straight line : Y = mX + c
    X --> X value
    Y --> Y value
    m --> Slope
    c --> Intercept
    p1(x1, y1), p2(x2,y2); Slope, m = y2 - y1 / x2 - x1; Intercept, c = y2 - (m * x2)
    p1(2, 7), p2(3,9); Slope, m = 9 - 7 / 3 - 2 = 2; Intercept, c = 9 - (2 * 3) = 9 - 6 = 3
    2.Polynomial model : Equation : y = b0 + b1 * x1 + b2 * exp(x1, 2)
    3.Logistic Regression
    4.Support Vector Machine
    5.K-Means Clustering
    ...
6.2. Supervised Learning Models
6.3. Unsupervised Learning Models
6.4. How to choose the right Machine Learning Model
    Model Selection : in machine learning is the proces of choosing the best suited ;odel for a particular problem, Selecting a model depends on various factors such as the dataset, task, nature of the model etc.
    Models can be selected based on : 
        Type of Data available : 
            Images & Videos - CNN (convolutional neural network) DL Model (Deep Learning Model)
            Text data & Speech data - RNN (recurrent neural network) DL Model (Deep Learning Model)
            Numerical data - SVM, Logistic Regression, Decision trees, etc.
        Based on the task we need to carry out :
            Classification tasks - SVM, Logistic Regression, Decision trees, etc.
                you need to classify something so the result (is or not)
                    Logistic Regression : when you have binary classification (two classes)
                    Support Vector Machines : when the dataset is very small and the are no outliers in th data
                        when there are a lot of outplays in the data cannot work well also when the data is to large because the processing time
            Regression tasks - Linear Regression, Random Forest, Polynomial Regression, etc.
                you try to find some value (price or temperature)
            Clustering tasks - K-means Clustering, Hierarchical Clustering
                you try to group the dataset based on the similarity
    Cross Validation :
        divide the data to multi part
        make multi iteration and use for each iteration a part as test data
        calculate the accuracy
        Dataset => data1,data2,data3
        iteration1 => data1,data2,data3 as test , accuracy is 88% for model1, accuracy  is 80% for model2
        iteration2 => data1,data2 as test,data3 , accuracy is 84% for model1, accuracy  is 81% for model2
        iteration3 => data1 as test,data2,data3 , accuracy is 80% for model1, accuracy  is 82% for model2
        accuracy score for model1 = 84%, accuracy score for model2 = 81%
        model1 is more performed
6.5. Overfitting in Machine Learning | Causes for Overfitting and its Prevention
    Overfitting : refers to a model that models the training data too well, Overfitting happens when a model learns the detail and noise in the training dataset to the extent that it negatively impacts the performance of the model
    Over Fit vs Good Fit
    Causes for Overfitting :
        Less Data
        Increased Complexity of the model
        More number of layers in neural network
    Preventing Overfitting by :
        Using more data
        Reduce he number of layers in the neural network
        Early Stopping
        Bias - Variance Tradeoff
        Use Dropouts
6.6. Underfitting in Machine Learning | Causes for Underfitting and its Prevention
    Underfitting : happends when the model does not learn enough from the data, Underfitting occurs when machine learning model cannot capture the underlying trend of the data
    Optimal vs Underfit
    Under Fit vs Good Fit
    Causes for Underfitting
        Choosing a wrong model
        Less complexity of the model
        Less variance but high bias
    Prevent Underfitting by :
        Choosing the correct model appropriate for the problem
        Increasing the complexity of the model
        More number of parameters to the model
        Bias - Variance Tradeoff
6.7. Bias Variance Tradeoff
    Bias : is the difference between the average prediction of our model and the correct value which we are trying to predict
    Variance : is the amount that the estimate of the target function of the target function will change if different training data was used
    Problem statement : identify an appropriate model to predict the height of a person when their weight is given
    Plot on training data : Underfitting / Overfitting
        when we use Underfitting we get very high bias but very low variance
        when we use Overfitting we get very less bias but very high variance
    Techniaues to have better bias - variance tradeoff :
        Good Model Selection
        Regularization
        Dimensionality Reduction
        Ensembke methods
6.8. Loss Function in Machine Learning
    Loss function : measures how far an estimated value is from its true value, It is helpfull to determine which model performs better and which parameters are better
    Loss = 1/n * sum(exp(Yi-^Yi, 2)) = Low Loss value = High Accuracy 
    Types of loss function : 
        Cross Entropy Loss
        Squared Error Loss
        KL Divergence
6.9. Model Evaluation in Machine Learning | Accuracy score | Mean Squared Error
    Work Flow of a ML Project
        Data -> Data Pre Processing -> Data Analysis -> Train Test split -> XGBoost Regressor -> Evaluation
    Evaluation metric for Supervised Learning
        Evaluation metric for Classification : Accuracy score
        Evaluation metric for Regression : Mean Absolute Error
    Accuracy score
        In Classification Accuracy Score is the ration of number of correct predictions to the total number of input data points
        Accuracy Score = Number of correct predictions / Total number of input data points * 100 %
    Mean Squared Error
        Mean Squared Error measures the avrege of the squares of the errors that is, the average squared difference between the estimated values and the actual value
        MSE = 1/n * sum(exp(Yi-^Yi, 2))
6.10. Model Parameters and Hyperparameters | Weights & Bias | Learning Rate & Epochs
    Parameters
        Model Parameters : 
            These are the parameters of the model that can be determined by training with training data
            These can be considered as internal parameters
            e.g. , Weights / Bias / Y = w*X + b / Y = w1*X1 + w2*X2 + w3*X3 + b
                X - feature or input variable
                Y - target or output variable
                w - weight
                b - bias
                e.g. , Applicant's Details, Input Data or feature or X : Name, Degree, College, C, C++, Python, Height, Weight, No. of Backlogs; Output Data or target or Y : select the applicant to hire to job
                - Weights :  Weight decides how much influence the input will have on the output
                we select the column important to select an applicant (Degree, College, C, C++, Python, No. of Backlogs)
                we assign some particular numerical value to columns  zero for not important column (Name, Height, Weight)
                we make diffrent value for each column defining the weighted of each column
                we take into considiration the positive and negative impact, negative value for column with negative impact and positive value for positive impact
                Name 0, Degree 2, College 3, C 5, C++ 6, Python 7, Height 0, Weight 0, No. of Backlogs -2
                - Bias : Bias is the offset value given to the model Bias is used to shift the model in a particular direction, It is similar to a Y-intercept. 'b' is equal to 'Y' when all the feature values are zero
        Hyperparameters :
            These are the parameters whose values control the learning process
            These are ajustable parameters used to obtain an optimal model
            These are External parameters
            e.g. , Learning Rate / Number of Epochs
                - Learning Rate : The Learning Rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function
                - Number of Epochs : Number of Epochs represents the number of times the model iterates over the entire dataset
6.11. Gradient Descent in Machine Learning
    Model Optimozation
        Optimization refers to determining best parameters for a model such that the loss function of the model decreases, as a result of wich the model can predict more accurately
    Gradient Descent
        Y is Loss function (J) and X is Weight (W)
        Global Minimum
            is the value with the minimum loss function the weight of this value is the better one
            start with random initial value
        w = w - L * dw
        b = b - L * db
        w -> weight
        b -> bais
        L -> Learning Rate
        dw --> Partial Derivative of loss function with respect to w
        db --> Partial Derivative of loss function with respect to b
